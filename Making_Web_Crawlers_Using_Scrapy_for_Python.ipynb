{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Making Web Crawlers Using Scrapy for Python"
      ],
      "metadata": {
        "id": "7IpaJCaTNEu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like an overview of web scraping in Python, take DataCamp's Web Scraping with Python course.\n",
        "\n",
        "In this tutorial, you will learn how to use Scrapy which is a Python framework using which you can handle large amounts of data! You will learn Scrapy by building a web scraper for AliExpress.com which is an e-commerce website. Let's get scrapping!\n",
        "\n",
        "A basic HTML and CSS knowledge will help you understand this tutorial with greater ease and speed. Read this article for a fresher on HTML and CSS.\n",
        "\n",
        "Scrapy Overview\n",
        "\n",
        "Web scraping has become an effective way of extracting information from the web for decision making and analysis. It has become an essential part of the data science toolkit. Data scientists should know how to gather data from web pages and store that data in different formats for further analysis.\n",
        "\n",
        "Any web page you see on the internet can be crawled for information and anything visible on a web page can be extracted [2]. Every web page has its own structure and web elements that because of which you need to write your web crawlers/spiders according to the web page being extracted.\n",
        "\n",
        "Scrapy provides a powerful framework for extracting the data, processing it and then save it.\n",
        "\n",
        "Scrapy uses spiders, which are self-contained crawlers that are given a set of instructions [1]. In Scrapy it is easier to build and scale large crawling projects by allowing developers to reuse their code.\n",
        "\n",
        "Scrapy Vs. BeautifulSoup\n",
        "\n",
        "In this section, you will have an overview of one of the most popularly used web scraping tool called BeautifulSoup and its comparison to Scrapy.\n",
        "\n",
        "Scrapy is a Python framework for web scraping that provides a complete package for developers without worrying about maintaining code.\n",
        "\n",
        "Beautiful Soup is also widely used for web scraping. It is a Python package for parsing HTML and XML documents and extract data from them. It is available for Python 2.6+ and Python 3.\n",
        "\n",
        "Here are some differences between them in a nutshell:\n",
        "\n",
        "Scrapy\tBeautifulSoup\n",
        "Functionality\t---\n",
        "Scrapy is the complete package for downloading web pages, processing them and save it in files and databases\tBeautifulSoup is basically an HTML and XML parser and requires additional libraries such as requests, urlib2 to open URLs and store the result [6]\n",
        "Learning Curve\t---\n",
        "Scrapy is a powerhouse for web scraping and offers a lot of ways to scrape a web page. It requires more time to learn and understand how Scrapy works but once learned, eases the process of making web crawlers and running them from just one line of command. Becoming an expert in Scrapy might take some practice and time to learn all functionalities.\tBeautifulSoup is relatively easy to understand for newbies in programming and can get smaller tasks done in no time\n",
        "Speed and Load\t---\n",
        "Scrapy can get big jobs done very easily. It can crawl a group of URLs in no more than a minute depending on the size of the group and does it very smoothly as it uses Twister which works asynchronously (non-blocking) for concurrency.\tBeautifulSoup is used for simple scraping jobs with efficiency. It is slower than Scrapy if you do not use multiprocessing.\n",
        "Extending functionality\t---\n",
        "Scrapy provides Item pipelines that allow you to write functions in your spider that can process your data such as validating data, removing data and saving data to a database. It provides spider Contracts to test your spiders and allows you to create generic and deep crawlers as well. It allows you to manage a lot of variables such as retries, redirection and so on.\tIf the project does not require much logic, BeautifulSoup is good for the job, but if you require much customization such as proxys, managing cookies, and data pipelines, Scrapy is the best option.\n",
        "Information: Synchronous means that you have to wait for a job to finish to start a new job while Asynchronous means you can move to another job before the previous job has finished"
      ],
      "metadata": {
        "id": "e98KoGHPNHA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Python 3.0 (and onwards) installed, if you are using anaconda, you can use conda to install scrapy. Write the following command in anaconda prompt:\n",
        "\n",
        "conda install -c conda-forge scrapy\n",
        "\n",
        "To install anaconda, look at these DataCamp tutorials for Mac and Windows.\n",
        "\n",
        "Alternatively, you can use Python Package Installer pip. This works for Linux, Mac, and Windows:\n",
        "\n",
        "pip install scrapy\n"
      ],
      "metadata": {
        "id": "lPOYAqWhNWRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapy Shell\n",
        "\n",
        "Scrapy also provides a web-crawling shell called as Scrapy Shell, that developers can use to test their assumptions on a siteâ€™s behavior. Let us take a web page for tablets at AliExpress e-commerce website. You can use the Scrapy shell to see what components the web page returns and how you can use them to your requirements.\n",
        "\n",
        "Open your command line and write the following command:\n",
        "\n",
        "scrapy shell\n",
        "\n",
        "If you are using anaconda, you can write the above command at the anaconda prompt as well. Your output on the command line or anaconda prompt will be something like this:"
      ],
      "metadata": {
        "id": "Zb9qL4MMNYDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to run a crawler on the web page using the fetch command in the Scrapy shell. A crawler or spider goes through a webpage downloading its text and metadata.\n",
        "\n",
        "fetch(https://pt.aliexpress.com/category/201005406/special-store.html)\n",
        "\n",
        "Note: Always enclose URL in quotes, both single and double quotes work\n",
        "\n",
        "The output will be as follows:\n",
        "...............................\n",
        "The crawler returns a response which can be viewed by using the view(response) command on shell:\n",
        "\n",
        "view(response)\n",
        "\n",
        "And the web page will be opened in the default browser."
      ],
      "metadata": {
        "id": "PebXqMeoNbqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using CSS Selectors for Extraction\n",
        "\n",
        "You can extract this using the element attributes or the css selector like classes. Write the following in the Scrapy shell to extract the product name:\n",
        "\n",
        "response.css(\".product::text\").extract_first()"
      ],
      "metadata": {
        "id": "3Kq6XADaNioP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using XPath for Extraction\n",
        "\n",
        "XPath is a query language for selecting nodes in an XML document [7]. You can navigate through an XML document using XPath. Behind the scenes, Scrapy uses Xpath to navigate to HTML document items. The CSS selectors you used above are also converted to XPath, but in many cases, CSS is very easy to use. But you should know how the XPath in Scrapy works.\n",
        "\n",
        "Go to your Scrapy Shell and write fetch(https://pt.aliexpress.com/category/201005406/special-store.html/) the same way as before. Try out the following code snippets [3]:\n",
        "\n",
        "response.xpath('/html').extract()\n",
        "\n",
        "This will show you all the code under the <html> tag. / means direct child of the node. If you want to get the <div> tags under the html tag you will write [3]:\n",
        "\n",
        "response.xpath('/html//div').extract()\n",
        "\n",
        "For XPath, you must learn to understand the use of / and // to know how to navigate through child and descendent nodes. Here is a helpful tutorial for XPath Nodes and some examples to try out.\n",
        "\n",
        "If you want to get all <div> tags, you can do it by drilling down without using the /html [3]:\n",
        "\n",
        "response.xpath(\"//div\").extract()\n",
        "\n",
        "You can further filter your nodes that you start from and reach your desired nodes by using attributes and their values. Below is the syntax to use classes and their values.\n",
        "\n",
        "response.xpath(\"//div[@class='quote']/span[@class='text']\").extract()\n",
        "\n",
        "response.xpath(\"//div[@class='quote']/span[@class='text']/text()\").extract()\n",
        "\n",
        "Use text() to extract all text inside nodes"
      ],
      "metadata": {
        "id": "as1Iaqs9Notn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Scrapy project and Custom Spider\n",
        "\n",
        "Web scraping can be used to make an aggregator that you can use to compare data. For example, you want to buy a tablet, and you want to compare products and prices together you can crawl your desired pages and store in an excel file. Here you will be scraping aliexpress.com for tablets information.\n",
        "\n",
        "Now, you will create a custom spider for the same page. First, you need to create a Scrapy project in which your code and results will be stored. Write the following command in the command line or anaconda prompt.\n",
        "\n",
        "scrapy startproject aliexpress"
      ],
      "metadata": {
        "id": "63dj3eHlNta-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a hidden folder in your default python or anaconda installation. aliexpress will be the name of the folder. You can give any name. You can view the folder contents directly through explorer. Following is the structure of the folder:\n",
        "\n"
      ],
      "metadata": {
        "id": "75DklTrJNxiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have created the project you will change to the newly created directory and write the following command:\n",
        "\n",
        "[scrapy genspider aliexpress_tablets](https://pt.aliexpress.com/category/201005406/special-store.html)"
      ],
      "metadata": {
        "id": "fg4zGF5kN0dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a template file named aliexpress_tablets.py in the spiders directory as discussed above. The code in that file is as below:"
      ],
      "metadata": {
        "id": "dEZJY1b4N3oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class AliexpressTabletsSpider(scrapy.Spider):\n",
        "    name = 'aliexpress_tablets'\n",
        "    allowed_domains = ['aliexpress.com']\n",
        "    start_urls = ['https://www.aliexpress.com/category/200216607/tablets.html']\n",
        "\n",
        "\n",
        "    def parse(self, response):\n",
        "         pass\n"
      ],
      "metadata": {
        "id": "UTBn9xqRN6ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code you can see name, allowed_domains, sstart_urls and a parse function.\n",
        "\n",
        "name: Name is the name of the spider. Proper names will help you keep track of all the spider's you make. Names must be unique as it will be used to run the spider when scrapy crawl name_of_spider is used.\n",
        "allowed_domains (optional): An optional python list, contains domains that are allowed to get crawled. Request for URLs not in this list will not be crawled. This should include only the domain of the website (Example: aliexpress.com) and not the entire URL specified in start_urls otherwise you will get warnings.\n",
        "start_urls: This requests for the URLs mentioned. A list of URLs where the spider will begin to crawl from, when no particular URLs are specified [4]. So, the first pages downloaded will be those listed here. The subsequent Request will be generated successively from data contained in the start URLs [4].\n",
        "parse(self, response): This function will be called whenever a URL is crawled successfully. It is also called the callback function. The response (used in Scrapy shell) returned as a result of crawling is passed in this function, and you write the extraction code inside it!\n",
        "Information: You can use BeautifulSoup inside parse() function of the Scrapy spider to parse the html document.\n",
        "\n",
        "Note: You can extract data through css selectors using response.css() as discussed in scrapy shell section but also using XPath (XML) that allows you to access child elements. You will see the example of response.xpath() in the code edited in pass() function.\n",
        "\n",
        "You will make changes to the aliexpress_tablet.py file. I have added another URL in start_urls. You can add the extraction logic to the pass() function as below:"
      ],
      "metadata": {
        "id": "p0pmbUvCOAiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class AliexpressTabletsSpider(scrapy.Spider):\n",
        "    name = 'aliexpress_tablets'\n",
        "    allowed_domains = ['aliexpress.com']\n",
        "    start_urls = ['https://www.aliexpress.com/category/200216607/tablets.html',\n",
        "                 'https://www.aliexpress.com/category/200216607/tablets/2.html?site=glo&g=y&tag=']\n",
        "\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "        print(\"procesing:\"+response.url)\n",
        "        #Extract data using css selectors\n",
        "        product_name=response.css('.product::text').extract()\n",
        "        price_range=response.css('.value::text').extract()\n",
        "        #Extract data using xpath\n",
        "        orders=response.xpath(\"//em[@title='Total Orders']/text()\").extract()\n",
        "        company_name=response.xpath(\"//a[@class='store $p4pLog']/text()\").extract()\n",
        "\n",
        "        row_data=zip(product_name,price_range,orders,company_name)\n",
        "\n",
        "        #Making extracted data row wise\n",
        "        for item in row_data:\n",
        "            #create a dictionary to store the scraped info\n",
        "            scraped_info = {\n",
        "                #key:value\n",
        "                'page':response.url,\n",
        "                'product_name' : item[0], #item[0] means product in the list and so on, index tells what value to assign\n",
        "                'price_range' : item[1],\n",
        "                'orders' : item[2],\n",
        "                'company_name' : item[3],\n",
        "            }\n",
        "\n",
        "            #yield or give the scraped info to scrapy\n",
        "            yield scraped_info\n"
      ],
      "metadata": {
        "id": "1RcMlqjxOGMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information: zip() takes n number of iterables and returns a list of tuples. ith element of the tuple is created using the ith element from each of the iterables. [8]\n",
        "\n",
        "The yield keyword is used whenever you are defining a generator function. A generator function is just like a normal function except it uses yield keyword instead of return. The yield keyword is used whenever the caller function needs a value and the function containing yield will retain its local state and continue executing where it left off after yielding value to the caller function. Here yield gives the generated dictionary to Scrapy which will process and save it!\n",
        "\n",
        "Now you can run the spider:\n",
        "\n",
        "scrapy crawl aliexpress_tablets\n",
        "\n"
      ],
      "metadata": {
        "id": "WtEcKddkOJE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting data\n",
        "\n",
        "You will need data to be presented as a CSV or JSON so that you can further use the data for analysis. This section of the tutorial will take you through how you can save CSV and JSON file for this data.\n",
        "\n",
        "To save a CSV file, open settings.py from the project directory and add the following lines:"
      ],
      "metadata": {
        "id": "bN6es2SAOQ2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FEED_FORMAT=\"csv\"\n",
        "FEED_URI=\"aliexpress.csv\"\n"
      ],
      "metadata": {
        "id": "WThzRPoROS-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "     + JSON\n",
        "     + CSV\n",
        "     + JSON Lines\n",
        "     + XML\n"
      ],
      "metadata": {
        "id": "bPH63OQMOUZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEED_URI [5]: This gives the location of the file. You can store a file on your local file storage or an FTP as well.\n",
        "Scrapy's Feed Export can also add a timestamp and the name of spider to your file name, or you can use these to identify a directory in which you want to store.\n",
        "\n",
        "%(time)s: gets replaced by a timestamp when the feed is being created [5]\n",
        "%(name)s: gets replaced by the spider name [5]\n",
        "For Example:\n",
        "\n",
        "Store in FTP using one directory per spider [5]:\n",
        "ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json\n",
        "\n",
        "The Feed changes you make in settings.py will apply to all spiders in the project. You can also set custom settings for a particular spider that will override the settings in the settings.py file."
      ],
      "metadata": {
        "id": "u-Rd6kwJOYc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class AliexpressTabletsSpider(scrapy.Spider):\n",
        "    name = 'aliexpress_tablets'\n",
        "    allowed_domains = ['aliexpress.com']\n",
        "    start_urls = ['https://www.aliexpress.com/category/200216607/tablets.html',\n",
        "                 'https://www.aliexpress.com/category/200216607/tablets/2.html?site=glo&g=y&tag=']\n",
        "\n",
        "    custom_settings={ 'FEED_URI': \"aliexpress_%(time)s.json\",\n",
        "                       'FEED_FORMAT': 'json'}\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "        print(\"procesing:\"+response.url)\n",
        "        #Extract data using css selectors\n",
        "        product_name=response.css('.product::text').extract()\n",
        "        price_range=response.css('.value::text').extract()\n",
        "        #Extract data using xpath\n",
        "        orders=response.xpath(\"//em[@title='Total Orders']/text()\").extract()\n",
        "        company_name=response.xpath(\"//a[@class='store $p4pLog']/text()\").extract()\n",
        "\n",
        "        row_data=zip(product_name,price_range,orders,company_name)\n",
        "\n",
        "        #Making extracted data row wise\n",
        "        for item in row_data:\n",
        "            #create a dictionary to store the scraped info\n",
        "            scraped_info = {\n",
        "                #key:value\n",
        "                'page':response.url,\n",
        "                'product_name' : item[0], #item[0] means product in the list and so on, index tells what value to assign\n",
        "                'price_range' : item[1],\n",
        "                'orders' : item[2],\n",
        "                'company_name' : item[3],\n",
        "            }\n",
        "\n",
        "            #yield or give the scraped info to Scrapy\n",
        "            yield scraped_info\n"
      ],
      "metadata": {
        "id": "D4GWMsKmOc2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify your aliexpress_tablets.py as belows\n",
        "\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class AliexpressTabletsSpider(scrapy.Spider):\n",
        "    name = 'aliexpress_tablets'\n",
        "    allowed_domains = ['aliexpress.com']\n",
        "    start_urls = ['https://www.aliexpress.com/category/200216607/tablets.html']\n",
        "\n",
        "\n",
        "    custom_settings={ 'FEED_URI': \"aliexpress_%(time)s.csv\",\n",
        "                       'FEED_FORMAT': 'csv'}\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "        print(\"procesing:\"+response.url)\n",
        "        #Extract data using css selectors\n",
        "        product_name=response.css('.product::text').extract()\n",
        "        price_range=response.css('.value::text').extract()\n",
        "        #Extract data using xpath\n",
        "        orders=response.xpath(\"//em[@title='Total Orders']/text()\").extract()\n",
        "        company_name=response.xpath(\"//a[@class='store $p4pLog']/text()\").extract()\n",
        "\n",
        "        row_data=zip(product_name,price_range,orders,company_name)\n",
        "\n",
        "        #Making extracted data row wise\n",
        "        for item in row_data:\n",
        "            #create a dictionary to store the scraped info\n",
        "            scraped_info = {\n",
        "                #key:value\n",
        "                'page':response.url,\n",
        "                'product_name' : item[0], #item[0] means product in the list and so on, index tells what value to assign\n",
        "                'price_range' : item[1],\n",
        "                'orders' : item[2],\n",
        "                'company_name' : item[3],\n",
        "            }\n",
        "\n",
        "            #yield or give the scraped info to scrapy\n",
        "            yield scraped_info\n",
        "\n",
        "\n",
        "            NEXT_PAGE_SELECTOR = '.ui-pagination-active + a::attr(href)'\n",
        "            next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()\n",
        "            if next_page:\n",
        "                yield scrapy.Request(\n",
        "                response.urljoin(next_page),\n",
        "                callback=self.parse)\n"
      ],
      "metadata": {
        "id": "dJ_dhIrrOhDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code:\n",
        "\n",
        "you first extracted the link of the next page using next_page = response.css(NEXT_PAGE_SELECTOR).extract_first() and then if the variable next_page gets a link and is not empty, it will enter the if body.\n",
        "\n",
        "response.urljoin(next_page): The parse() method will use this method to build a new url and provide a new request, which will be sent later to the callback. [9]\n",
        "\n",
        "After receiving the new URL, it will scrape that link executing the for body and again look for the next page. This will continue until it doesn't get a next page link.\n",
        "\n",
        "Here you might want to sit back and enjoy your spider scraping all the pages. The above spider will extract from all subsequent pages. That will be a lot of scraping! But your spider will do it! Below you can see the size of the file has reached 1.1MB."
      ],
      "metadata": {
        "id": "0R9Y23y5O4Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapy does it for you!\n",
        "\n",
        "In this tutorial, you have learned about Scrapy, how it compares to BeautifulSoup, Scrapy Shell and how to write your own spiders in Scrapy. Scrapy handles all the heavy load of coding for you, from creating project files and folders till handling duplicate URLs it helps you get heavy-power web scraping in minutes and provides you support for all common data formats that you can further input in other programs. This tutorial will surely help you understand Scrapy and its framework and what you can do with it. To become a master in Scrapy, you will need to go through all the fantastic functionalities it has to provide, but this tutorial has made you capable of scraping groups of web pages in an efficient way.\n",
        "\n",
        "For further reading, you can refer to Offical Scrapy Docs.\n"
      ],
      "metadata": {
        "id": "QmrcrYneO_w-"
      }
    }
  ]
}